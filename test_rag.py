import streamlit as st
import os
import sys
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import OllamaLLM
from langchain_ollama.embeddings import OllamaEmbeddings
from langchain_postgres.vectorstores import PGVector

# Importy do adowania i dzielenia dokument贸w
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter  # Poprawiony import

# --- 1. Konfiguracja (Stae) ---

CONNECTION_STRING = "postgresql+psycopg2://postgres:1234@localhost:5433/Baza"
COLLECTION_NAME = "rag_app_docs"

# Inicjalizacja modeli
try:
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    llm = OllamaLLM(model="llama2")
    print("INFO: Modele Ollama zaadowane pomylnie.")
except Exception as e:
    st.error(f"KRYTYCZNY BD: Nie mo偶na poczy si z Ollama. Upewnij si, 偶e jest uruchomiona. Bd: {e}")
    sys.exit()


# --- 2. Funkcja do przetwarzania i indeksowania pliku ---

def process_and_embed_file(file_path):
    """Wczytuje PDF, dzieli go i zapisuje wektory w bazie danych."""

    st.info("Krok 1/4: Wczytywanie dokumentu PDF...")
    loader = PyPDFLoader(file_path)
    docs = loader.load()

    if not docs:
        st.error("Nie udao si wczyta 偶adnych stron z tego PDFa.")
        return False

    st.info(f"Krok 2/4: Dzielenie tekstu na fragmenty (zaadowano {len(docs)} stron)...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(docs)

    if not chunks:
        st.error("Nie udao si wyodrbni tekstu z tego PDFa.")
        return False

    st.info("Krok 3/4: Tworzenie poczenia z baz wektorow...")
    try:
        vectorstore = PGVector(
            connection=CONNECTION_STRING,
            embeddings=embeddings,
            collection_name=COLLECTION_NAME,
            create_extension=False
        )

        st.info("Krok 3.5/4: Czyszczenie starej kolekcji (jeli istnieje)...")
        vectorstore.delete_collection()

        # --- !!! POPRAWKA TUTAJ !!! ---
        st.info("Krok 3.6/4: Tworzenie nowej, pustej kolekcji...")
        vectorstore.create_collection()
        # --- ---------------------- ---

        st.info(f"Krok 4/4: Dodawanie {len(chunks)} fragment贸w tekstu do bazy...")
        vectorstore.add_documents(documents=chunks)

        return True
    except Exception as e:
        st.error(f"Wystpi bd podczas dodawania dokument贸w do bazy: {e}")
        import traceback
        traceback.print_exc()
        return False


# --- 3. G贸wny Interfejs Streamlit ---
st.title("Lokalny System RAG ")
st.markdown("Wgraj dokument PDF, aby m贸c zadawa mu pytania.")

# --- Panel Boczny (Sidebar) do wgrywania plik贸w ---
with st.sidebar:
    st.header("1. Wgraj Dokument")
    uploaded_file = st.file_uploader("Wybierz plik PDF...", type=["pdf"])

    if uploaded_file:
        temp_dir = "temp_uploads"
        os.makedirs(temp_dir, exist_ok=True)
        temp_path = os.path.join(temp_dir, uploaded_file.name)

        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        st.info("Rozpoczynanie przetwarzania pliku...")

        file_id = f"processed_{uploaded_file.name}"

        if file_id not in st.session_state:
            with st.spinner("Przetwarzanie pliku... (adowanie, dzielenie, embedding)"):
                success = process_and_embed_file(temp_path)
                if success:
                    st.session_state[file_id] = True
                    st.session_state['file_ready'] = True
                    st.success(f"Plik '{uploaded_file.name}' jest gotowy!")
                else:
                    st.error("Przetwarzanie pliku nie powiodo si.")
        else:
            st.success(f"Plik '{uploaded_file.name}' zosta ju偶 przetworzony.")
            st.session_state['file_ready'] = True

# --- G贸wny Interfejs (Czat) ---
st.header("2. Zadaj Pytanie")

user_question = st.text_input("Twoje pytanie:", placeholder="np. Kto jest klientem na fakturze 456?")

if st.button("Wylij Pytanie"):
    if not st.session_state.get('file_ready'):
        st.error("Prosz najpierw wgra i przetworzy dokument w panelu bocznym.")
    elif not user_question:
        st.warning("Prosz wpisa pytanie.")
    else:
        with st.spinner("Myl...  (Przeszukuj baz i generuj odpowied藕)"):
            try:
                vectorstore_read = PGVector(
                    connection=CONNECTION_STRING,
                    embeddings=embeddings,
                    collection_name=COLLECTION_NAME,
                    create_extension=False
                )

                retriever = vectorstore_read.as_retriever(search_kwargs={'k': 3})

                template = """
                Odpowiedz na pytanie bazujc wycznie na poni偶szym kontekcie.
                Jeli kontekst nie zawiera odpowiedzi, napisz "Nie wiem".

                Kontekst:
                {context}

                Pytanie:
                {question}
                """
                prompt = ChatPromptTemplate.from_template(template)

                chain = (
                        {"context": retriever, "question": RunnablePassthrough()}
                        | prompt
                        | llm
                        | StrOutputParser()
                )

                response = chain.invoke(user_question)
                st.success("Odpowied藕 wygenerowana:")
                st.markdown(response)

            except Exception as e:
                st.error(f"Wystpi bd podczas generowania odpowiedzi: {e}")
                import traceback

                traceback.print_exc()